'use client'

import { useState, useRef, useEffect } from 'react'
import { ArrowLeft, Moon, Sun, Mic, MicOff } from 'lucide-react'
import Link from 'next/link'
import Image from 'next/image'
import { motion, AnimatePresence } from 'framer-motion'
import { useLanguage } from '../contexts/LanguageContext'
import { useTheme } from '../contexts/ThemeContext'
import { translate, translateVoiceChat } from '../utils/translations'

// íŒŒì¼ ìƒë‹¨ì— íƒ€ì… ì •ì˜ ì¶”ê°€
interface SpeechRecognitionEvent {
  resultIndex: number
  results: {
    [key: number]: {
      [key: number]: {
        transcript: string
      }
      isFinal: boolean
    }
  }
}

interface SpeechRecognitionErrorEvent {
  error: string
}

interface SpeechRecognition extends EventTarget {
  continuous: boolean
  interimResults: boolean
  lang: string
  start: () => void
  stop: () => void
  onstart: () => void
  onend: () => void
  onerror: (event: SpeechRecognitionErrorEvent) => void
  onresult: (event: SpeechRecognitionEvent) => void
}

declare global {
  interface Window {
    SpeechRecognition: new () => SpeechRecognition
    webkitSpeechRecognition: new () => SpeechRecognition
  }
}

export default function VoiceChatPage() {
  const { language } = useLanguage()
  const { isDarkMode, toggleDarkMode } = useTheme()
  
  // ìŒì„± ëŒ€í™” ê´€ë ¨ ìƒíƒœ ì¶”ê°€
  const [isListening, setIsListening] = useState(false)
  const [isTalking, setIsTalking] = useState(false)
  const [transcript, setTranscript] = useState("")
  const [isProcessing, setIsProcessing] = useState(false)
  const [conversationActive, setConversationActive] = useState(false)
  const [isPaused, setIsPaused] = useState(false)
  
  // Refs
  const audioContextRef = useRef<AudioContext | null>(null)
  const analyserRef = useRef<AnalyserNode | null>(null)
  const mediaStreamRef = useRef<MediaStream | null>(null)
  const silenceTimeoutRef = useRef<NodeJS.Timeout | null>(null)
  const recognitionRef = useRef<SpeechRecognition | null>(null)
  const audioPlayerRef = useRef<HTMLAudioElement | null>(null)
  const isListeningRef = useRef(isListening)
  const isProcessingRef = useRef(isProcessing)
  const isPausedRef = useRef(isPaused)
  const conversationActiveRef = useRef(conversationActive)

  // ìŒì„± ê°ì§€ ì„¤ì •
  const SILENCE_THRESHOLD = 15 // ë¬µìŒ ì„ê³„ê°’
  const SILENCE_DURATION = 1500 // ë¬µìŒ ì§€ì† ì‹œê°„ (1.5ì´ˆ)

  // ìŒì„± íŒŒí˜• ë°”ë“¤ì˜ ì• ë‹ˆë©”ì´ì…˜ì„ ìœ„í•œ ì„¤ì •
  const bars = Array.from({ length: 30 }) // 30ê°œì˜ íŒŒí˜• ë°”
  const getRandomHeight = () => Math.random() * 50 + 10 // 10-60px ì‚¬ì´ì˜ ëœë¤ ë†’ì´

  // SpeechRecognition ì„¤ì • í•¨ìˆ˜ ìˆ˜ì •
  const setupSpeechRecognition = () => {
    try {
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition
      if (!SpeechRecognition) {
        throw new Error('Speech Recognition is not supported in this browser')
      }

      const recognition = new SpeechRecognition()
      
      recognition.lang = language === 'ko' ? 'ko-KR' : 'en-US'
      recognition.continuous = false  // ì—°ì† ì¸ì‹ ë¹„í™œì„±í™”
      recognition.interimResults = true
      
      recognition.onstart = () => {
        console.log('ìŒì„± ì¸ì‹ ì‹œì‘')
        setIsListening(true)
      }
      
      recognition.onresult = (event) => {
        const current = event.resultIndex
        const transcript = event.results[current][0].transcript
        
        console.log('ìŒì„± ì¸ì‹ ì¤‘:', transcript)
        setTranscript(transcript)
        
        if (event.results[current].isFinal) {
          console.log('ğŸ¤ ìµœì¢… ìŒì„± ì¸ì‹ ê²°ê³¼:', transcript)
          handleSpeechResult(transcript)
        }
      }
      
      recognition.onerror = (event) => {
        console.error('ìŒì„± ì¸ì‹ ì˜¤ë¥˜:', event.error)
        stopListening()
      }
      
      recognition.onend = () => {
        console.log('ìŒì„± ì¸ì‹ ì¢…ë£Œ')
        // ìŒì„± ì¸ì‹ì´ ì¢…ë£Œë˜ì—ˆì„ ë•Œ ìë™ìœ¼ë¡œ ì¬ì‹œì‘í•˜ì§€ ì•ŠìŒ
        // ëŒ€ì‹  handleSpeechResultì—ì„œ AI ì‘ë‹µ í›„ restartRecognitionì„ í˜¸ì¶œ
      }

      recognitionRef.current = recognition
      return recognition
    } catch (error) {
      console.error('Speech Recognition ì„¤ì • ì˜¤ë¥˜:', error)
      return null
    }
  }

  // handleSpeechResult í•¨ìˆ˜ ìˆ˜ì •
  const handleSpeechResult = async (text: string) => {
    if (!text.trim() || isProcessing) return
    
    console.log('ìŒì„± ì¸ì‹ ê²°ê³¼ ì²˜ë¦¬ ì‹œì‘, conversationActive:', conversationActiveRef.current);
    if (!conversationActiveRef.current) {
      console.log('ëŒ€í™”ê°€ í™œì„±í™”ë˜ì§€ ì•Šì•„ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ');
      return;
    }
    
    setIsProcessing(true)
    try {
      // ìŒì„± ì¸ì‹ ì¼ì‹œ ì¤‘ì§€ (but keep isListening true)
      if (recognitionRef.current) {
        try {
          recognitionRef.current.stop();
          setIsPaused(true);
          console.log('ìŒì„± ì¸ì‹ ì¼ì‹œ ì¤‘ì§€: recognition paused during AI response');
          console.log('isPaused ìƒíƒœë¥¼ trueë¡œ ì„¤ì •');
        } catch (err) {
          console.error('ìŒì„± ì¸ì‹ ì¤‘ì§€ ì¤‘ ì˜¤ë¥˜:', err);
        }
      }

      const response = await fetch('/api/voice-chat', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ text })
      })

      if (!response.ok) throw new Error('API ì‘ë‹µ ì˜¤ë¥˜')

      const audioBlob = await response.blob()
      await playAudio(audioBlob)

      console.log('AI ì‘ë‹µ ì¬ìƒ ì™„ë£Œ');

    } catch (error) {
      if (error instanceof Error) {
        console.warn('ìŒì„± ì²˜ë¦¬ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤:', error.message)
      }
    } finally {
      setIsProcessing(false)
      
      // isProcessingì´ falseë¡œ ì„¤ì •ëœ í›„ì— ì¬ì‹œì‘ ë¡œì§ ì‹¤í–‰
      if (conversationActiveRef.current) {
        console.log('finally ë¸”ë¡ì—ì„œ ì¬ì‹œì‘ ë¡œì§ ì‹¤í–‰, conversationActive:', conversationActiveRef.current);
        // ìƒíƒœ ì—…ë°ì´íŠ¸ê°€ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì´ë£¨ì–´ì§€ë¯€ë¡œ, ì•½ê°„ì˜ ì§€ì—° í›„ ì¬ì‹œì‘
        setTimeout(() => {
          console.log('finally ë¸”ë¡ì˜ íƒ€ì„ì•„ì›ƒì—ì„œ ì¬ì‹œì‘ ì‹œë„');
          console.log('ì¬ì‹œì‘ ì§ì „ ìƒíƒœ - conversationActive:', conversationActiveRef.current, 'isPaused:', isPaused);
          restartRecognition();
        }, 800); // 0.8ì´ˆë¡œ ì¤„ì„
      } else {
        console.log('ëŒ€í™”ê°€ í™œì„±í™”ë˜ì§€ ì•Šì•„ ì¬ì‹œì‘í•˜ì§€ ì•ŠìŒ, conversationActive:', conversationActiveRef.current);
      }
    }
  }

  // startListening í•¨ìˆ˜ ìˆ˜ì •
  const startListening = async () => {
    try {
      console.log('ëŒ€í™” ì‹œì‘í•˜ê¸° ë²„íŠ¼ í´ë¦­ë¨');
      setConversationActive(true);
      conversationActiveRef.current = true; // ì¦‰ì‹œ ref ì—…ë°ì´íŠ¸
      console.log('conversationActive ìƒíƒœë¥¼ trueë¡œ ì„¤ì • (ref:', conversationActiveRef.current, ')');
      // ê¸°ì¡´ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
      if (recognitionRef.current) {
        recognitionRef.current.stop()
        recognitionRef.current = null
      }

      // Speech Recognition ìƒˆë¡œ ì„¤ì • ë° ì‹œì‘
      const recognition = setupSpeechRecognition()
      if (recognition) {
        recognition.start()
      }

      // ì˜¤ë””ì˜¤ ë¶„ì„ ì„¤ì •
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
      mediaStreamRef.current = stream
      
      const audioContext = new AudioContext()
      const analyser = audioContext.createAnalyser()
      const source = audioContext.createMediaStreamSource(stream)
      
      analyser.fftSize = 2048
      source.connect(analyser)
      
      audioContextRef.current = audioContext
      analyserRef.current = analyser
      
      setIsListening(true)
      monitorAudioLevel()
      
    } catch (error) {
      console.error('ìŒì„± ì¸ì‹ ì‹œì‘ ì˜¤ë¥˜:', error)
      stopListening()
    }
  }

  const monitorAudioLevel = () => {
    if (!analyserRef.current) return

    const analyser = analyserRef.current
    const dataArray = new Uint8Array(analyser.frequencyBinCount)
    
    const checkLevel = () => {
      if (!isListening) return
      
      analyser.getByteFrequencyData(dataArray)
      const average = dataArray.reduce((acc, val) => acc + val, 0) / dataArray.length
      
      if (average > SILENCE_THRESHOLD) {
        // ìŒì„± ê°ì§€ë¨
        setIsTalking(true)
        if (silenceTimeoutRef.current) {
          clearTimeout(silenceTimeoutRef.current)
          silenceTimeoutRef.current = null
        }
      } else if (isTalking) {
        // ë¬µìŒ ê°ì§€
        if (!silenceTimeoutRef.current) {
          silenceTimeoutRef.current = setTimeout(() => {
            handleSpeechEnd()
          }, SILENCE_DURATION)
        }
      }
      
      requestAnimationFrame(checkLevel)
    }
    
    checkLevel()
  }

  const handleSpeechEnd = async () => {
    setIsTalking(false)
    // transcriptê°€ ì´ë¯¸ handleSpeechResultì—ì„œ ì²˜ë¦¬ë˜ì—ˆìœ¼ë¯€ë¡œ
    // ì—¬ê¸°ì„œëŠ” ìƒíƒœë§Œ ì´ˆê¸°í™”
    setTranscript("")
  }

  // stopListening í•¨ìˆ˜ ìˆ˜ì •
  const stopListening = () => {
    try {
      setConversationActive(false); // ëŒ€í™” í™œì„±í™” ìƒíƒœë¥¼ falseë¡œ ì„¤ì •
      conversationActiveRef.current = false; // ì¦‰ì‹œ ref ì—…ë°ì´íŠ¸
      console.log('ëŒ€í™” ì¢…ë£Œ: conversationActiveë¥¼ falseë¡œ ì„¤ì •');
      setIsProcessing(false) // ë¨¼ì € ì²˜ë¦¬ ìƒíƒœë¥¼ falseë¡œ ì„¤ì •

      // 1. í˜„ì¬ ì¬ìƒ ì¤‘ì¸ ì˜¤ë””ì˜¤ ì¤‘ì§€ - ì—ëŸ¬ ì²˜ë¦¬ ì¶”ê°€
      if (audioPlayerRef.current) {
        try {
          audioPlayerRef.current.pause();
          audioPlayerRef.current.src = '';
          audioPlayerRef.current = null;
        } catch (err) {
          // ì˜¤ë””ì˜¤ ì¤‘ì§€ ì¤‘ ì—ëŸ¬ëŠ” ë¬´ì‹œ
        }
      }

      // 2. Web Speech Recognition ì¤‘ì§€
      if (recognitionRef.current) {
        try {
          recognitionRef.current.stop()
          // ë¹ˆ í•¨ìˆ˜ë¡œ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì´ˆê¸°í™”
          recognitionRef.current.onend = () => {};
          recognitionRef.current.onresult = () => {};
          recognitionRef.current.onerror = () => {};
          recognitionRef.current = null
        } catch (err) {
          // Recognition ì¤‘ì§€ ì¤‘ ì—ëŸ¬ëŠ” ë¬´ì‹œ
        }
      }

      // 3. ë¯¸ë””ì–´ ìŠ¤íŠ¸ë¦¼ íŠ¸ë™ ì¤‘ì§€
      if (mediaStreamRef.current) {
        try {
          mediaStreamRef.current.getTracks().forEach(track => {
            track.stop()
            mediaStreamRef.current?.removeTrack(track)
          })
          mediaStreamRef.current = null
        } catch (err) {
          // ë¯¸ë””ì–´ ìŠ¤íŠ¸ë¦¼ ì¤‘ì§€ ì¤‘ ì—ëŸ¬ëŠ” ë¬´ì‹œ
        }
      }

      // 4. AudioContext ì •ë¦¬
      if (audioContextRef.current) {
        try {
          audioContextRef.current.close()
          audioContextRef.current = null
        } catch (err) {
          // AudioContext ì¤‘ì§€ ì¤‘ ì—ëŸ¬ëŠ” ë¬´ì‹œ
        }
      }

      // 5. AnalyserNode ì—°ê²° í•´ì œ
      if (analyserRef.current) {
        try {
          analyserRef.current.disconnect()
          analyserRef.current = null
        } catch (err) {
          // Analyser ì—°ê²° í•´ì œ ì¤‘ ì—ëŸ¬ëŠ” ë¬´ì‹œ
        }
      }

      // 6. íƒ€ì´ë¨¸ ì •ë¦¬
      if (silenceTimeoutRef.current) {
        clearTimeout(silenceTimeoutRef.current)
        silenceTimeoutRef.current = null
      }

      // 7. ìƒíƒœ ì´ˆê¸°í™”
      setIsListening(false)
      setIsTalking(false)
      setTranscript("")

      console.log('ìŒì„± ëŒ€í™”ê°€ ì•ˆì „í•˜ê²Œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.')
    } catch (error) {
      console.warn('ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì¼ë¶€ ì‘ì—…ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤:', error)
    }
  }

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
  useEffect(() => {
    return () => {
      stopListening()
    }
  }, [])

  // í˜ì´ì§€ ì´ë™ ì‹œ ì •ë¦¬ë¥¼ ìœ„í•œ ì¶”ê°€
  useEffect(() => {
    const handleBeforeUnload = () => {
      stopListening()
    }

    window.addEventListener('beforeunload', handleBeforeUnload)
    return () => {
      window.removeEventListener('beforeunload', handleBeforeUnload)
    }
  }, [])

  // playAudio í•¨ìˆ˜ ìˆ˜ì •
  const playAudio = async (audioBlob: Blob): Promise<void> => {
    return new Promise((resolve, reject) => {
      try {
        // ì´ì „ ì˜¤ë””ì˜¤ ì •ë¦¬
        if (audioPlayerRef.current) {
          try {
            audioPlayerRef.current.pause();
            audioPlayerRef.current.src = '';
          } catch (err) {
            // ì´ì „ ì˜¤ë””ì˜¤ ì¤‘ì§€ ì¤‘ ì—ëŸ¬ëŠ” ë¬´ì‹œ
          }
        }

        const audioUrl = URL.createObjectURL(audioBlob)
        const audio = new Audio(audioUrl)
        audioPlayerRef.current = audio;
        
        audio.onended = () => {
          URL.revokeObjectURL(audioUrl)
          audioPlayerRef.current = null;
          resolve();
        }

        audio.onerror = () => {
          URL.revokeObjectURL(audioUrl)
          audioPlayerRef.current = null;
          reject(new Error('ì˜¤ë””ì˜¤ ì¬ìƒì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.'))
        }

        audio.play().catch(err => {
          reject(new Error('ì˜¤ë””ì˜¤ ì¬ìƒì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'))
        })
      } catch (error) {
        reject(new Error('ì˜¤ë””ì˜¤ ì¬ìƒ ì¤€ë¹„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'))
      }
    })
  }

  // ìƒíƒœ ë³€ê²½ ì‹œ ref ì—…ë°ì´íŠ¸
  useEffect(() => {
    isListeningRef.current = isListening
  }, [isListening])

  useEffect(() => {
    isProcessingRef.current = isProcessing
  }, [isProcessing])

  useEffect(() => {
    isPausedRef.current = isPaused
  }, [isPaused])

  useEffect(() => {
    conversationActiveRef.current = conversationActive;
  }, [conversationActive]);

  // restartRecognition í•¨ìˆ˜ ìˆ˜ì •
  const restartRecognition = () => {
    console.log('ì¬ì‹œì‘: Speech Recognition ì¬ì„¤ì • ì‹œë„');
    console.log('í˜„ì¬ ìƒíƒœ - conversationActive:', conversationActiveRef.current, 'isPaused:', isPaused, 'isProcessing:', isProcessing);
    
    if (!conversationActiveRef.current) {
      console.log('ëŒ€í™”ê°€ í™œì„±í™”ë˜ì§€ ì•Šì•„ ì¬ì‹œì‘í•˜ì§€ ì•ŠìŒ');
      return;
    }
    
    // ìƒíƒœ ì—…ë°ì´íŠ¸ë¥¼ ë¨¼ì € ìˆ˜í–‰
    setIsPaused(false);
    
    // ì•½ê°„ì˜ ì§€ì—° í›„ ì‹¤ì œ ì¬ì‹œì‘ ìˆ˜í–‰ (ìƒíƒœ ì—…ë°ì´íŠ¸ê°€ ë°˜ì˜ë˜ë„ë¡)
    setTimeout(() => {
      try {
        console.log('ì¬ì‹œì‘ íƒ€ì„ì•„ì›ƒ ì‹¤í–‰ë¨');
        
        // ê¸°ì¡´ ì¸ìŠ¤í„´ìŠ¤ê°€ ìˆìœ¼ë©´ ì •ë¦¬
        if (recognitionRef.current) {
          try {
            recognitionRef.current.stop();
            recognitionRef.current.onend = () => {};
            recognitionRef.current.onresult = () => {};
            recognitionRef.current.onerror = () => {};
            recognitionRef.current = null;
            console.log('ê¸°ì¡´ recognition ì¸ìŠ¤í„´ìŠ¤ ì •ë¦¬ë¨');
          } catch (err) {
            console.log('ê¸°ì¡´ recognition ì¸ìŠ¤í„´ìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜:', err);
          }
        }
        
        // ìƒˆ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ì‹œì‘
        const recognition = setupSpeechRecognition();
        if (recognition) {
          console.log('ìƒˆ recognition ì¸ìŠ¤í„´ìŠ¤ ìƒì„±ë¨, ì‹œì‘ ì‹œë„...');
          recognition.start();
          recognitionRef.current = recognition;
          setIsListening(true); // ëª…ì‹œì ìœ¼ë¡œ isListening ì„¤ì •
          console.log('ì¬ì‹œì‘: Speech Recognition ì‹œì‘ë¨');
        } else {
          console.warn('ì¬ì‹œì‘: Speech Recognition ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨');
        }
      } catch (error) {
        console.error('ìŒì„± ì¸ì‹ ì¬ì‹œì‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:', error);
      }
    }, 200); // 0.2ì´ˆë¡œ ì¤„ì„
  };

  return (
    <div className={`min-h-screen flex flex-col ${isDarkMode ? 'bg-gray-900 text-white' : 'bg-gray-100 text-gray-900'}`}>
      {/* í—¤ë” */}
      <header className={`fixed top-0 left-0 right-0 z-50 border-b ${
        isDarkMode ? 'bg-gray-900 border-gray-700' : 'bg-white border-gray-200'
      }`}>
        <div className="max-w-5xl mx-auto px-4">
          <div className="flex items-center justify-between py-4">
            <Link 
              href="/chat" 
              className={`p-2 rounded-full ${
                isDarkMode 
                  ? 'hover:bg-gray-700 text-white' 
                  : 'hover:bg-gray-100 text-gray-900'
              } flex items-center gap-2`}
            >
              <ArrowLeft className="w-5 h-5" />
              <span>Back</span>
            </Link>
            <button
              onClick={toggleDarkMode}
              className={`p-2 rounded-full ${
                isDarkMode ? 'hover:bg-gray-700 text-white' : 'hover:bg-gray-100 text-gray-900'
              }`}
            >
              {isDarkMode ? <Sun className="w-5 h-5" /> : <Moon className="w-5 h-5" />}
            </button>
          </div>
        </div>
      </header>

      {/* ë©”ì¸ ì»¨í…ì¸  */}
      <main className="flex-1 max-w-5xl mx-auto w-full mt-20 p-8">
        <div className="flex flex-col items-center justify-center min-h-[calc(100vh-160px)]">
          <div className="w-32 h-32 relative rounded-full overflow-hidden mb-8 shadow-xl">
            <Image
              src="/profile.png"
              alt={translate('name', language)}
              layout="fill"
              className="object-cover"
            />
          </div>

          {/* ìŒì„± íŒŒí˜• ì• ë‹ˆë©”ì´ì…˜ - ìƒíƒœì— ë”°ë¼ ë‹¤ë¥´ê²Œ í‘œì‹œ */}
          <div className="flex items-center justify-center gap-1 mb-8 h-32">
            {bars.map((_, i) => (
              <motion.div
                key={i}
                className={`w-1 rounded-full ${
                  isTalking 
                    ? 'bg-green-500' 
                    : isDarkMode ? 'bg-blue-400' : 'bg-blue-500'
                }`}
                animate={{
                  height: isTalking 
                    ? [getRandomHeight() * 1.5, getRandomHeight() * 1.5] 
                    : isListening 
                      ? [getRandomHeight(), getRandomHeight()]
                      : "20px"
                }}
                transition={{
                  duration: isTalking ? 0.3 : 1.5,
                  repeat: Infinity,
                  repeatType: "reverse",
                  delay: i * 0.05,
                }}
              />
            ))}
          </div>

          {/* ìƒíƒœ ë©”ì‹œì§€ */}
          <div className={`text-center max-w-md mx-auto ${isDarkMode ? 'text-gray-300' : 'text-gray-700'}`}>
            <h2 className="text-2xl font-bold mb-4">
              {isListening 
                ? isTalking 
                  ? translateVoiceChat('recognizingVoice', language) || "ìŒì„±ì„ ì¸ì‹í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
                  : translateVoiceChat('pleaseSpeak', language) || "ë§ì”€í•´ ì£¼ì„¸ìš”"
                : translate('voiceChatTitle', language).replace('{name}', `${translate('name', language)}${translate('cloneTitle', language)}`)}
            </h2>
            <p className="text-lg opacity-75">
              {isListening 
                ? translateVoiceChat('autoVoiceDetection', language) || "ìë™ìœ¼ë¡œ ìŒì„±ì„ ê°ì§€í•˜ì—¬ ëŒ€í™”í•©ë‹ˆë‹¤"
                : translateVoiceChat('speakFreely', language) || "ììœ ë¡­ê²Œ ë§ì”€í•´ì£¼ì„¸ìš”. ìë™ìœ¼ë¡œ ìŒì„±ì„ ì¸ì‹í•˜ì—¬ ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."}
            </p>
          </div>

          {/* ëŒ€í™” ì‹œì‘/ì¢…ë£Œ ë²„íŠ¼ */}
          <motion.button
            onClick={isListening ? stopListening : startListening}
            className={`mt-12 px-8 py-4 rounded-full text-white font-medium
              ${isListening 
                ? 'bg-red-500 hover:bg-red-600' 
                : isDarkMode 
                  ? 'bg-blue-600 hover:bg-blue-700' 
                  : 'bg-blue-500 hover:bg-blue-600'
              } transform transition-all duration-200 hover:scale-105 shadow-lg`}
            whileTap={{ scale: 0.95 }}
          >
            {isListening ? translateVoiceChat('endConversation', language) || 'ëŒ€í™” ì¢…ë£Œí•˜ê¸°' : translateVoiceChat('startConversation', language) || 'ëŒ€í™” ì‹œì‘í•˜ê¸°'}
          </motion.button>
        </div>
      </main>
    </div>
  )
} 